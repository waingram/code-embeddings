{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to represent code in Doc2Vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from code_embeddings.utils import tokenize\n",
    "from javalang import tokenizer\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_code_dir = Path('test_data')\n",
    "train_code_dir = Path('training_data')\n",
    "models_dir = Path('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 records\nTraining corpus size: 99088\n"
     ]
    }
   ],
   "source": [
    "def read_train_corpus():\n",
    "    for i, file in enumerate(train_code_dir.glob('./java_projects/**/*.java')):\n",
    "        if not file.is_file():  # oddly, some of these are not files\n",
    "            continue\n",
    "        tokens = None\n",
    "        with file.open() as f:\n",
    "            try:\n",
    "                code = f.read()\n",
    "                tokens = list(tokenizer.tokenize(code))\n",
    "                tokens = [token for t in tokens for token in t.value.split(\" \")]\n",
    "            except Exception as e:\n",
    "                # print(\"Error: %s\" % e)\n",
    "                pass\n",
    "        if tokens:\n",
    "            yield TaggedDocument(tokens, [file.name])\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Processed %s records\" % i)\n",
    "        if i > 100000:\n",
    "            break\n",
    "\n",
    "\n",
    "train_corpus = list(read_train_corpus())\n",
    "print(\"Training corpus size: %s\" % len(train_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=0,  # training algorithm: 1 = PV-DM, 0 = PV-DBOW\n",
    "                min_count=2,  # Ignores all words with total frequency lower than this\n",
    "                max_vocab_size=None,\n",
    "                workers=multiprocessing.cpu_count(),  # number of cores\n",
    "                epochs=20,  # Number of iterations (epochs) over the corpus\n",
    "                vector_size=50,\n",
    "                dbow_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 57min 35s, sys: 1min 25s, total: 2h 59min 1s\nWall time: 49min 37s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(models_dir / \"github-java-vectors.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eleIndex', 0.8072444796562195),\n ('peekCount', 0.7907533049583435),\n ('pendingCallbacks', 0.7730069160461426),\n ('actualTargetIndex', 0.7661998867988586),\n ('srcByteCount', 0.7639729976654053),\n ('countLoadedFileds', 0.7629091739654541),\n ('updateSimpleLine', 0.7598205804824829),\n ('do', 0.7588541507720947),\n ('patIdxEnd', 0.7573974132537842),\n ('scanWhitespace', 0.7559382319450378)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('counter', 0.7916474342346191),\n ('last', 0.7904374599456787),\n ('listsSize', 0.7793775796890259),\n ('firstHalf', 0.7724998593330383),\n ('increment', 0.7703016996383667),\n ('cnt', 0.7674715518951416),\n ('total', 0.7653804421424866),\n ('numHistory', 0.763133704662323),\n ('numTicks', 0.7631007432937622),\n ('bitmapLength', 0.7610165476799011)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IllegalArgumentException', 0.8226144313812256),\n ('IllegalStateException', 0.8146161437034607),\n ('\"null', 0.7925406694412231),\n ('\"DhDsaExchange', 0.782356321811676),\n ('allowed\"', 0.781735897064209),\n ('throw', 0.7796768546104431),\n ('\"gbeanType', 0.7724958062171936),\n ('\\\\\"bundleContext\\\\\"', 0.7692555785179138),\n ('cannot', 0.7691822052001953),\n ('\\\\\"bundle\\\\\"', 0.7661920189857483)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('NullPointerException')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_corpus():\n",
    "    for programming_language in test_code_dir.glob('./Java'):\n",
    "        if not programming_language.is_dir():\n",
    "            continue\n",
    "        for programming_task in programming_language.glob('./*'):\n",
    "            if not programming_task.is_dir():\n",
    "                continue\n",
    "            for implementation in programming_task.glob('./*'):\n",
    "                with implementation.open() as f:\n",
    "                    # tokens = tokenize(f.read())\n",
    "                    code = f.read()\n",
    "                    try:\n",
    "                        tokens = list(tokenizer.tokenize(code))\n",
    "                        tokens = [token for t in tokens for token in t.value.split(\" \")]\n",
    "                    except:\n",
    "                        pass\n",
    "                yield TaggedDocument(tokens, [implementation.name])\n",
    "                \n",
    "                \n",
    "test_corpus = list(read_test_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
