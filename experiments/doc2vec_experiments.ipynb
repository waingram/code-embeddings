{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to represent code in Doc2Vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import gensim.models.doc2vec\n",
    "import regex\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from javalang import tokenizer\n",
    "\n",
    "from code_embeddings.utils import tokenize\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_code_dir = Path('../test_data')\n",
    "train_code_dir = Path('../training_data')\n",
    "models_dir = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec parameters\n",
    "vector_size = 50\n",
    "window_size = 16\n",
    "min_count = 5\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "epochs = 20\n",
    "dm = 0  # 0 = dbow; 1 = dmpv\n",
    "worker_count = multiprocessing.cpu_count()  # number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_methods(code):\n",
    "    \"\"\"Parse Java files into separate methods\n",
    "\n",
    "        :param code: Java code to parse.\n",
    "        :rtype: map\n",
    "    \"\"\"\n",
    "    pattern = r'(?:(?:public|private|static|protected)\\s+)*\\s*[\\w\\<\\>\\[\\]]+\\s+\\w+\\s*\\([^{]+({(?:[^{}]+\\/\\*.*?\\*\\/|[^{}]+\\/\\/.*?$|[^{}]+|(?1))*+})'\n",
    "    scanner = regex.finditer(pattern, code, regex.MULTILINE)\n",
    "    return map(lambda match: match.group(0), scanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records\n"
     ]
    }
   ],
   "source": [
    "def read_train_corpus():\n",
    "    count = -1\n",
    "    dict = {}\n",
    "    for i, file in enumerate(train_code_dir.glob('./java_projects/**/*.java')):\n",
    "        if not file.is_file():  # oddly, some of these are not files\n",
    "            continue\n",
    "        with file.open() as f:\n",
    "            try:\n",
    "                code = f.read()\n",
    "                methods = split_methods(code)\n",
    "                for method in methods:\n",
    "                    tokens = list(tokenizer.tokenize(method))\n",
    "                    tokens = [token for t in tokens for token in t.value.split(\" \")]\n",
    "                    if tokens:\n",
    "                        count += 1\n",
    "                        path = file.__fspath__()\n",
    "                        dict[count] = path\n",
    "                        yield TaggedDocument(tokens, [count])\n",
    "            except tokenizer.LexerError as e:\n",
    "                # print(\"%s: %s\" % (type(e).__name__, e))\n",
    "                pass\n",
    "            except UnicodeDecodeError as e:\n",
    "                # print(\"%s: %s\" % (type(e).__name__, e))\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(\"%s: %s\" % (type(e).__name__, e))\n",
    "                pass\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Processed %s records\" % i)\n",
    "        if i > 100000:\n",
    "            break\n",
    "\n",
    "    # also include test corpus in training! \n",
    "    for programming_language in test_code_dir.glob('./Java'):\n",
    "        if not programming_language.is_dir():\n",
    "            continue\n",
    "        for programming_task in programming_language.glob('./*'):\n",
    "            if not programming_task.is_dir():\n",
    "                continue\n",
    "            for implementation in programming_task.glob('./*'):\n",
    "                with implementation.open() as f:\n",
    "                    try:\n",
    "                        code = f.read()\n",
    "                        tokens = list(tokenizer.tokenize(code))\n",
    "                        tokens = [token for t in tokens for token in t.value.split(\" \")]\n",
    "                        if tokens:\n",
    "                            count += 1\n",
    "                            path = file.__fspath__()\n",
    "                            dict[count] = path\n",
    "                            yield TaggedDocument(tokens, [count])\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning: %s\" % e)\n",
    "                        pass\n",
    "\n",
    "    # save map to csv\n",
    "    with open(str(models_dir / 'java_doc_map.csv'), 'w', newline='') as csvfile:\n",
    "        w = csv.writer(csvfile)\n",
    "        for key, val in dict.items():\n",
    "            w.writerow([key, val])\n",
    "                \n",
    "   \n",
    "\n",
    "\n",
    "%time train_corpus = list(read_train_corpus())\n",
    "print(\"Training corpus size: %s\" % len(train_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_corpus():\n",
    "    color_val = 0\n",
    "    for programming_language in test_code_dir.glob('./Java'):\n",
    "        if not programming_language.is_dir():\n",
    "            continue\n",
    "        for programming_task in programming_language.glob('./*'):\n",
    "            if not programming_task.is_dir():\n",
    "                continue\n",
    "            color_val += 1\n",
    "            for implementation in programming_task.glob('./*'):\n",
    "                with implementation.open() as f:\n",
    "                    try:\n",
    "                        code = f.read()\n",
    "                        tokens = list(tokenizer.tokenize(code))\n",
    "                        tokens = [token for t in tokens for token in t.value.split(\" \")]\n",
    "                        if tokens:\n",
    "                            yield TaggedDocument(tokens, [programming_task.name, implementation.name, programming_language.name, color_val])\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning: %s\" % e)\n",
    "                        pass\n",
    "                \n",
    "                \n",
    "test_corpus = list(read_test_corpus())\n",
    "print(\"Test corpus size: %s\" % len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = Doc2Vec(train_corpus,\n",
    "                vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                min_count=min_count, \n",
    "                sample=sampling_threshold,\n",
    "                negative=negative_size,\n",
    "                dbow_words=1,\n",
    "                epochs=epochs, \n",
    "                dm=dm,\n",
    "                workers=worker_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(models_dir / \"github-java-vectors.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('NullPointerException')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "test1 = [doc for doc in test_corpus if doc.tags[1] == 'sieve-of-eratosthenes-6.java'][0]\n",
    "test2 = [doc for doc in test_corpus if doc.tags[1] == 'sieve-of-eratosthenes-6.java'][0]\n",
    "\n",
    "test1_vector = model.infer_vector(test1.words, steps=200)\n",
    "test2_vector = model.infer_vector(test2.words, steps=200)\n",
    "\n",
    "dist = scipy.spatial.distance.cosine(test1_vector, test2_vector)\n",
    "print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "with open(str(models_dir / 'java_doc_map.csv'), newline='') as csvfile:\n",
    "    r = csv.reader(csvfile)\n",
    "    for row in r:\n",
    "        dict[int(row[0])] = row[1]\n",
    "\n",
    "sims = model.docvecs.most_similar([test1_vector])\n",
    "print(dict[0])\n",
    "f = Path(dict[sims[0][0]])\n",
    "print(f.name)\n",
    "print(\"Similarity: %s\" % sims[0][1])\n",
    "with f.open() as fin:\n",
    "    print(fin.read(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [{'name': doc.tags[0], 'vec': model.infer_vector(doc.words, steps=200), 'color': doc.tags[3]} for doc in test_corpus]\n",
    "num_colors = docs[-1]['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (32, 16)\n",
    "\n",
    "tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "twoDimVecs = tsne.fit_transform([doc['vec'] for doc in docs])\n",
    "\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "cNorm = colors.Normalize(vmin=0, vmax=num_colors-1)\n",
    "scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for doc, twoDimVec in zip(docs, twoDimVecs):\n",
    "    ax.scatter(twoDimVec[0], twoDimVec[1], color=scalarMap.to_rgba(doc['color']))\n",
    "    plt.annotate(doc['name'],\n",
    "                 xy=(twoDimVec[0], twoDimVec[1]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
